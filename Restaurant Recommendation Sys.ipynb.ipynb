{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "i-l-1esCIV9b",
        "outputId": "fed06900-afce-45de-caf1-bf0ac6625544"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'split'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-09669f148167>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata_source_mapping\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATA_SOURCE_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_url_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_source_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdownload_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_url_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'zomato-bangalore-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F926506%2F1567895%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240625%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240625T160109Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D964f4678a2761d05991ca81f0f5e61f813187550ea41bc7861beaaff283ef1d77868303ec8e83643da413b3df88afabd78667d15966648f0c1f0fa0f93604342291755b87eb4c7f616231b2eb29001a487024e179e7b70856dd61a5c79d002eb3b90f77f7f4d129763aa7e4e5b117bd23eb23a0c2f758345b4dd936c449e3412c7ef1eb77f7a08ad60db04b8ef6fdaebc39bdbf86f50dd6805eec41c78b18efceabf5628ef571c2d7b2b69392aeb52919178eda2f805ec9b2abe6cf2eaeba48bc9597982b02b8a1fffd482e1b1d726027b5ce36248c91c3a07b24da93e350f9bd5210b39eac42aca8ee731daa09c0744601d8a10aebd8e231a234ed0e8ed8b88'\n",
        "DATA_SOURCE_MAPPING = os.path.expanduser(DATA_SOURCE_MAPPING),'UTF=8'\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5471xZ9JIV9g"
      },
      "source": [
        "## How the Restaurant Recommendation System Works?\n",
        "\n",
        "One of the type of programming in ML is data filtering and data cleaning which is technically used with syntaxes in python, some of the types related to recommendation system are reataurant recommendation system. Recommendation systems are defined as a technique which used to recommend or regenerate something. For example: The methods used for recommend a cuisine of a resraurant.\n",
        "It is of two types-\n",
        "\n",
        "\n",
        "1. Content-based filtering\n",
        "2. Collaborative filtering\n",
        "https://www.bing.com/ck/a?!&&p=14d006f2a075f6ddJmltdHM9MTcxOTQ0NjQwMCZpZ3VpZD0yMzllNmViMC1lNDljLTYzMDMtMDcyNC03YTNiZTUzNDYyZWYmaW5zaWQ9NTUyMA&ptn=3&ver=2&hsh=3&fclid=239e6eb0-e49c-6303-0724-7a3be53462ef&psq=Recommendation+system+types+&u=a1aHR0cHM6Ly9jZW9iZWUuZGV2L2Jsb2cvdW5kZXJzdGFuZGluZy1yZWNvbW1lbmRhdGlvbi1zeXN0ZW1zLXR5cGVzLWFwcGxpY2F0aW9ucy1hbmQtY2hhbGxlbmdlcyM6fjp0ZXh0PVR5cGVzJTIwb2YlMjBSZWNvbW1lbmRhdGlvbiUyMFN5c3RlbXMlMjAxJTIwQ29sbGFib3JhdGl2ZSUyMEZpbHRlcmluZyUzQSUyMFRoaXMsRmlsdGVyaW5nJTIwdG8lMjBwcm92aWRlJTIwbW9yZSUyMGFjY3VyYXRlJTIwYW5kJTIwZGl2ZXJzZSUyMHJlY29tbWVuZGF0aW9ucy4&ntb=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMiCpKbvIV9i"
      },
      "source": [
        "I will start the task of Restaurant Recommendation System by importing the necessary Python Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-06-24T07:52:52.757595Z",
          "iopub.status.busy": "2024-06-24T07:52:52.757209Z",
          "iopub.status.idle": "2024-06-24T07:52:52.764575Z",
          "shell.execute_reply": "2024-06-24T07:52:52.763638Z",
          "shell.execute_reply.started": "2024-06-24T07:52:52.757563Z"
        },
        "id": "Y-057nxGIV9j",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msb\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXl8cDPKIV9l"
      },
      "source": [
        "Now, I will load and read the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-24T07:52:52.767025Z",
          "iopub.status.busy": "2024-06-24T07:52:52.766635Z",
          "iopub.status.idle": "2024-06-24T07:52:59.147799Z",
          "shell.execute_reply": "2024-06-24T07:52:59.146919Z",
          "shell.execute_reply.started": "2024-06-24T07:52:52.766991Z"
        },
        "id": "dQ46SGf9IV9m",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "zomato_app=pd.read_csv(\"/content/Dataset .csv\")\n",
        "zomato_app.head() # prints the first 5 rows of the dataset\n",
        "dataset = zomato_app.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOtpKdeIV9n"
      },
      "source": [
        "Now the next step is data cleaning and feature engineering for this step we need to do a lot of stuff with the data such as:\n",
        "\n",
        "1. Deleting Unnecessary Columns\n",
        "2. Removing the Duplicates\n",
        "3. Remove the NaN values from the dataset\n",
        "4. Changing the column names\n",
        "5. Data Transformations\n",
        "6. Data Cleaning\n",
        "7. Adjust the column names\n",
        "Now, let’s perform all the above steps in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "execution": {
          "iopub.execute_input": "2024-06-24T07:52:59.150381Z",
          "iopub.status.busy": "2024-06-24T07:52:59.150077Z",
          "iopub.status.idle": "2024-06-24T07:53:55.382236Z",
          "shell.execute_reply": "2024-06-24T07:53:55.381332Z",
          "shell.execute_reply.started": "2024-06-24T07:52:59.150349Z"
        },
        "id": "9pdAoe2pIV9o",
        "outputId": "ca2577a1-fed0-4944-e56b-4a9e325d8abb",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-181-964714d2b6bc>, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-181-964714d2b6bc>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    zomato_app = zomato_app.drop(['url','dish_liked', 'phone'],axis=1): #Dropping the column \"dish_liked\", \"phone\", \"url\" and saving the new dataset as \"zomato\"\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#Deleting Unnnecessary Columns\n",
        "df = pd.dataframe(zomato_app)\n",
        "zomato_app = zomato_app.drop(['url','dish_liked', 'phone'],axis=1): #Dropping the column \"dish_liked\", \"phone\", \"url\" and saving the new dataset as \"zomato\"\n",
        "\n",
        "#Removing the Duplicates\n",
        "zomato_app.duplicated().sum()\n",
        "zomato_app.drop_duplicates(inplace=True)\n",
        "\n",
        "#Remove the NaN values from the dataset\n",
        "zomato_app.isnull().sum()\n",
        "zomato_app.dropna(how='any',inplace=True)\n",
        "\n",
        "#Changing the column names\n",
        "zomato_app = zomato_app.rename(columns={'approx_cost(for two people)':'cost','listed_in(type)':'type', 'listed_in(city)':'city'})\n",
        "\n",
        "#Some Transformations\n",
        "zomato['cost'] = zomato['cost'].astype(str) #Changing the cost to string\n",
        "zomato['cost'] = zomato['cost'].apply(lambda x: x.replace(',','.')) #Using lambda function to replace ',' from cost\n",
        "zomato['cost'] = zomato['cost'].astype(float)\n",
        "#Removing '/5' from Rates\n",
        "zomato_app = zomato_app.loc[zomato.rate !='NEW']\n",
        "zomato_app = zomato_app.loc[zomato.rate !='-'].reset_index(drop=True)\n",
        "remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\n",
        "zomato_app.rate = zomato_app.rate.apply(remove_slash).str.strip().astype('float')\n",
        "\n",
        "# Adjust the column names\n",
        "zomato_app.name = zomato.name.apply(lambda x:x.title())\n",
        "zomato_app.online_order.replace(('Yes','No'),(True, False),inplace=True)\n",
        "zomato_app.book_table.replace(('Yes','No'),(True, False),inplace=True)\n",
        "\n",
        "## Computing Mean Rating\n",
        "restaurants = list(zomato['name'].unique())\n",
        "zomato_app['Mean Rating'] = 0\n",
        "\n",
        "for i in range(len(restaurants)):\n",
        "    zomato['Mean Rating'][zomato['name'] == restaurants[i]] = zomato['rate'][zomato['name'] == restaurants[i]].mean()\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (1,5))\n",
        "zomato[['Mean Rating']] = scaler.fit_transform(zomato[['Mean Rating']]).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlraShvBIV9o"
      },
      "source": [
        "Now the next step is to perform some text preprocessing steps which include:\n",
        "\n",
        "1. Lower casing\n",
        "2. Removal of Punctuations\n",
        "3. Removal of Stopwords\n",
        "4. Removal of URLs\n",
        "5. Spelling correction\n",
        "\n",
        "Now let’s perform the above text preprocessing steps on the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "execution": {
          "iopub.execute_input": "2024-06-24T07:53:55.384782Z",
          "iopub.status.busy": "2024-06-24T07:53:55.384455Z",
          "iopub.status.idle": "2024-06-24T07:54:56.211938Z",
          "shell.execute_reply": "2024-06-24T07:54:56.210805Z",
          "shell.execute_reply.started": "2024-06-24T07:53:55.38475Z"
        },
        "id": "txnhZ-keIV9p",
        "outputId": "9df558d0-6b72-48a6-bcce-8b2feb7dd95c",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-191-6c821661bfd2>, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-191-6c821661bfd2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    zomato == zomato_app.align[[(\"reviews_list\")] = zomato_app[(\"reviews_list\")].str.lower()]\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "## Lower Casing\n",
        "zomato == zomato_app.align[[(\"reviews_list\")] = zomato_app[(\"reviews_list\")].str.lower()]\n",
        "\n",
        "## Removal of Punctuations\n",
        "zomato_app.head()\n",
        "\n",
        "## Removal of Puctuations\n",
        "import string\n",
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_punctuation(text))\n",
        "\n",
        "## Removal of Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_stopwords(text))\n",
        "\n",
        "## Removal of URLS\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_urls(text))\n",
        "\n",
        "zomato[['reviews_list', 'cuisines']].sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-24T07:54:56.213789Z",
          "iopub.status.busy": "2024-06-24T07:54:56.213477Z",
          "iopub.status.idle": "2024-06-24T07:54:56.243402Z",
          "shell.execute_reply": "2024-06-24T07:54:56.242531Z",
          "shell.execute_reply.started": "2024-06-24T07:54:56.21376Z"
        },
        "id": "AYyz3JEdIV9q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#RESTAURANT NAMES\n",
        "restaurant_names = list(zomato['name'].unique())\n",
        "def get_top_words(column, top_nu_of_words, nu_of_word):\n",
        "    vec = CountVectorizer(ngram_range= nu_of_word, stop_words='english')\n",
        "    bag_of_words = vec.fit_transform(column)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:top_nu_of_words]\n",
        "\n",
        "zomato=zomato_app.drop(['address','rest_type', 'type', 'menu_item'],axis=1)\n",
        "import pandas\n",
        "\n",
        "# Randomly sample 60% of your dataframe\n",
        "df_percent = zomato.sample(frac=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWZ63rJTIV9q"
      },
      "source": [
        "## TF-IDF Vectorization\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) used for determining statiscal document vectorization. This will give you a matrix where each column represents a word in the general vocabulary (all words that appear in at least one document) and each column represents a restaurant, as before.\n",
        "\n",
        "TF-IDF is the statistical method used for assessing the meaning of a word in a given document. Now, I will use the TF-IDF vectorization on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "execution": {
          "iopub.execute_input": "2024-06-24T07:54:56.244836Z",
          "iopub.status.busy": "2024-06-24T07:54:56.244525Z",
          "iopub.status.idle": "2024-06-24T07:57:20.62685Z",
          "shell.execute_reply": "2024-06-24T07:57:20.625861Z",
          "shell.execute_reply.started": "2024-06-24T07:54:56.244806Z"
        },
        "id": "zs8KikH0IV9r",
        "outputId": "42280a12-19cb-4310-8c57-30bd0e5948a8",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-196-f48355de095f>, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-196-f48355de095f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df_percent.set_index('Value', inplace=True):\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "df_percent.set_index('Value', inplace=True):\n",
        "df_percent.head()\n",
        "feature = data [\"Type\"].tolist()\n",
        "text.TfidfVectorizer(input=feature, stop_words = \"english\")\n",
        "tfidf_matrix = text.fit_transform(feature)\n",
        "similarity_matrix = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "indices = pd.Series(df_percent.index)\n",
        "\n",
        "# Creating tf-idf matrix\n",
        "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df_percent['reviews_list'])\n",
        "\n",
        "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy7HIWq0IV9r"
      },
      "source": [
        "Now the last step for creating a Restaurant Recommendation System is to write a function that will recommend restaurants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "execution": {
          "iopub.execute_input": "2024-06-24T07:57:20.628564Z",
          "iopub.status.busy": "2024-06-24T07:57:20.628238Z",
          "iopub.status.idle": "2024-06-24T07:57:20.886408Z",
          "shell.execute_reply": "2024-06-24T07:57:20.885525Z",
          "shell.execute_reply.started": "2024-06-24T07:57:20.628524Z"
        },
        "id": "8B13uLMmIV9r",
        "outputId": "80311901-53ea-43e6-c649-463fca5837e0",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    recommend_restaurant = []\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "from types import new_class\n",
        "     def recommend(name, cosine_similarities = cosine_similarities), (indices = indices), (df_percent = df_percent.html_escape()):\n",
        "\n",
        "    # Create a list to put top restaurants\n",
        "    recommend_restaurant = []\n",
        "\n",
        "    # Find the index of the hotel entered\n",
        "    idx = indices[indices == name].index[0]\n",
        "\n",
        "    # Find the restaurants with a similar cosine-sim value and order them from bigges number\n",
        "    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False)\n",
        "\n",
        "    # Extract top 30 restaurant indexes with a similar cosine-sim value\n",
        "    top30_indexes = list(score_series.iloc[0:31].index)\n",
        "\n",
        "    # Names of the top 30 restaurants\n",
        "    for each in top30_indexes:\n",
        "        recommend_restaurant.append(list(df_percent.index)[each])\n",
        "\n",
        "    # Creating the new data set to show similar restaurants\n",
        "    df_new = pd.DataFrame(columns=['cuisines', 'Mean Rating', 'cost'])\n",
        "\n",
        "    # Create the top 30 similar restaurants with some of their columns\n",
        "    for each in recommend_restaurant:\n",
        "        df_new = df_new.append(pd.DataFrame(df_percent[['cuisines','Mean Rating', 'cost']][df_percent.index == each].sample()))\n",
        "\n",
        "    # Drop the same named restaurants and sort only the top 10 by the highest rating\n",
        "    df_new = df_new.drop_duplicates(subset=['cuisines','Mean Rating', 'cost'], keep=False)\n",
        "    df_new = df_new.sort_values(by='Mean Rating', ascending=False).head(10)\n",
        "\n",
        "    print('TOP %s RESTAURANTS LIKE %s WITH SIMILAR REVIEWS: ' % (str(len(df_new)), name))\n",
        "\n",
        "    return df_new:\n",
        "      recommend('Pai Vihar')\n",
        "\n",
        "for each in top45_dishes:\n",
        "    print(each)\n",
        "    if len(df_new_dish) < 100:\n",
        "        df_new = df_new.append(pd.DataFrame(df_percent[['cuisines','dishes']][df.value_index == each].sample()))\n",
        "    else:\n",
        "        print('No similar dishes found')\n",
        "\n",
        "    elif len(df_new_dish) == 100:\n",
        "        break:\n",
        "\n",
        "    df_new_dish = df_new_dish.delete_duplicates(subset=['cuisines','dishes', 'menu'], keep=False)\n",
        "    df_new_dish = df_new_dish.drop_values(by='dishes', ascending=False).head(10)\n",
        "    print(top_k_score_menu, 'menu hooked')\n",
        "\n",
        "  #df_new_class.insert('zomato') and assert(\"recommendation system\"):\n",
        "\n",
        "import sys\n",
        "\n",
        "df_new, Recommendation_System_Zomato(n): # Return factorial\n",
        "    output = 1:\n",
        "\n",
        "    for i in range (1,n):\n",
        "        result = result * i\n",
        "    print [(\"Recommendation_System_Zomato\"),result]\n",
        "\n",
        "     return output:\n",
        "\n",
        "    print( \"Recommendation system Zomato\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcajb5OsIV9s"
      },
      "source": [
        "#### As as you can see that we got a fairly good output. So, I hope you liked this article on Machine Learning project on Restaurant Recommendation system with Python programming language. Feel free to ask your valuable questions in the comments section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6iQ_VivIV9s",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 926506,
          "sourceId": 1567895,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30066,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
