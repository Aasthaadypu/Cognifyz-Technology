{
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9643,
          "sourceType": "datasetVersion",
          "datasetId": 6660
        }
      ],
      "dockerImageVersionId": 46,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Restaurant Ratings"
      ],
      "metadata": {
        "_uuid": "93b7d96ea4a342b529d12cd15ebec38a3bf08661",
        "id": "ZquikJSCtsFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this analysis is to build a prediction model to predict whether a review on the restaurant is positive or negative. To do so, we will work on Restaurant Rating dataset, we will load it into predicitve algorithms Multinomial Naive Bayes, Bernoulli Naive Bayes and Logistic Regression. In the end, we hope to find a \"best\" model for predicting the rating.\n",
        "\n",
        "Dataset: [Dataset.csv]is a dataset from Cognifyz Technologies which consists of 1000 reviews on a restaurant.\n",
        "\n",
        "To build a model to predict if review is positive or negative, following steps are performed.\n",
        "\n",
        "* Importing Dataset\n",
        "* Preprocessing Dataset\n",
        "* Vectorization\n",
        "* Training and Classification\n",
        "* Analysis Conclusion"
      ],
      "metadata": {
        "_uuid": "c09ee4f9185b743274b9cbc808c2a46edd72a83f",
        "id": "iHeyl_BntsFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Dataset"
      ],
      "metadata": {
        "_uuid": "0b2828b368ecc03ab374142073170c7f7492634e",
        "id": "8rF1iny5tsFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Restaurant Review dataset using pandas library."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "A98yYPdutsFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "_uuid": "776c1d0ee74fc5508a4a91ec4f3556e107b8b455",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "QB0jK0cbtsFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/Dataset .csv', delimiter = '\\t', quoting = 3)"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "BA_hhp33tsFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Dataset"
      ],
      "metadata": {
        "_uuid": "e35ff1aefdc1a9a69887d85d802f0b4406d7dbaa",
        "id": "1dLrX2PLtsFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each review undergoes through a preprocessing step, where all the vague information is removed.\n",
        "\n",
        "* Removing the Stopwords, numeric and speacial charecters.\n",
        "* Normalizing each review using the approach of stemming."
      ],
      "metadata": {
        "_uuid": "aa144f3ed6026061078c88034d1f906180ea5453",
        "id": "rT03KWlftsFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "for i in range(0, 1000):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "_uuid": "7d73a4db8fe837094ff5e40c04919bf6047d1f3c",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "Rx5GAEA7tsFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization"
      ],
      "metadata": {
        "_uuid": "51cc3a8def8bb7eeb6508651b972efc753763aeb",
        "id": "Ufa6N2aktsFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the cleaned dataset, potential features are extracted and are converted to numerical format. The vectorization techniques are used to convert textual data to numerical format. Using vectorization, a matrix is created where each column represents a feature and each row represents an individual review."
      ],
      "metadata": {
        "_uuid": "484360e5b03c701261000dc5544a38efad4dfde8",
        "id": "l2OWdTb8tsFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model using CountVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1000)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, 1].values"
      ],
      "metadata": {
        "_uuid": "4e5cc79ee441495e83613276c7dab35f0ac1ddd2",
        "trusted": true,
        "id": "Z3tsRpt-tsFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Classification"
      ],
      "metadata": {
        "_uuid": "a606773ada1375cdabfa0fec62f4b976d74a5eb5",
        "id": "iykZs1DEtsFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further the data is splitted into training and testing set using Cross Validation technique. This data is used as input to classification algorithm.\n",
        "\n",
        "**Classification Algorithms:**\n",
        "\n",
        "Algorithms like Decision tree, Support Vector Machine, Logistic Regression, Naive Bayes were implemented and on comparing the evaluation metrics two of the algorithms gave better predictions than others.\n",
        "\n",
        "* Multinomial Naive Bayes\n",
        "* Bernoulli Naive Bayes\n",
        "* Logistic Regression"
      ],
      "metadata": {
        "_uuid": "703554381341fa910ae696b6d150df9e9879e5d2",
        "id": "YB6sIsx7tsFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
      ],
      "metadata": {
        "_uuid": "2b105697aba2280f3e9d8f9f691c63aa31e5a38d",
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "vdgY4r3jtsFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multinomial NB**"
      ],
      "metadata": {
        "_uuid": "ea86674ad05134e5b7962bffc63f173a0817accb",
        "id": "6mf76BxwtsFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multinomial NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB(alpha=0.1)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "metadata": {
        "_uuid": "ae32807740cd509e4353b65ac7b588ab7df3fc9f",
        "trusted": true,
        "id": "0FMqfNWztsFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bernoulli NB**"
      ],
      "metadata": {
        "_uuid": "fb1890535da8d5b47d13c517d9c8f502c74ff7ca",
        "id": "8hjRd-G3tsFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bernoulli NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB(alpha=0.8)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "metadata": {
        "_uuid": "3afba49eb67124353842111eec8a856b8b910822",
        "trusted": true,
        "id": "97Xjr2x1tsFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "_uuid": "b764217c7a6ba8b7acd8dcf72abb3b890d338a33",
        "id": "GBIDVDr9tsFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn import linear_model\n",
        "classifier = linear_model.LogisticRegression(C=1.5)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "metadata": {
        "_uuid": "ec65c31586cbcd91c8ece17895b067dd23e76adf",
        "trusted": true,
        "id": "F1_3aOVTtsFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis and Conclusion"
      ],
      "metadata": {
        "_uuid": "1b4ab0c5a0882e69b332bb3c607fe620e972942d",
        "id": "sDjZVMrYtsFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this study, an attempt has been made to classify sentiment analysis for restaurant reviews using machine learning techniques. Two algorithms namely Multinomial Naive Bayes and Bernoulli Naive Bayes are implemented.\n",
        "\n",
        "Evaluation metrics used here are accuracy, precision and recall.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "6be3a04b43930fc0da789bdf38de7ce25fa477e1",
        "id": "FKaaZCgftsFJ"
      }
    }
  ]
}