{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwixDrHmk3Dp"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import matplotlib.pylab as plt\n",
        "  from collections import Counter\n",
        "  import matplotlib as mpl\n",
        "  from sklearn import preprocessing\n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv('/content/Dataset .csv')\n",
        "df.head()\n",
        "\n",
        "# Create empty list to store recipe features\n",
        "features_all_list = []\n",
        "16\n",
        "17# Extract the features from each recipe (need a global list)\n",
        "for i in df.ingredients:\n",
        "    features_all_list += i\n",
        "\n",
        "# Remove duplicate features using default set behavior\n",
        "features = list( set(features_all_list) )\n",
        "\n",
        "len(features)\n",
        "\n",
        " onehot_ingredients = np.zeros((df.shape[0], len(features)))\n",
        "\n",
        " # Index the features (ingredients) alphabetically\n",
        " feature_lookup = sorted(features)\n",
        "\n",
        " # For each recipe look up ingredient position in the sorted ingredient list\n",
        " # If that ingredient exists, set the appropriate column equal to 1\n",
        " ## This will take 1-2 minutes to finish running\n",
        "for index, row in df.iterrows():\n",
        "    for ingredient in row['ingredients']:\n",
        "       onehot_ingredients[index, feature_lookup.index(ingredient)] = 1\n",
        "\n",
        "y = df.cuisine.values.reshape(-1,1)\n",
        "#Using the indices of the ingredients, we can reduce the amount of string matching required to one-hot encode the ingredients into binary features.\n",
        "\n",
        " # Create a dataframe\n",
        " df_features = pd.DataFrame(onehot_ingredients)\n",
        "\n",
        " # Create empty dictionary to store featureindex:columnname\n",
        " df = {}\n",
        "\n",
        " # For each feature, fetch the column name\n",
        " for i in range(len(features)):\n",
        "     df[df_features.columns[i]] = features[i]\n",
        "\n",
        "# Rename the features (stop using the index # and use the actual text)\n",
        "df_features = df_features.rename(columns=d)\n",
        "df_features.shape()\n",
        "#The shape of df_features is (39774, (6714) meaning we have 39774 recipes and 6714 unique ingredients.\n",
        "\n",
        "\n",
        "#CUISINE CLASSIFICATION\n",
        "#In order to classify with best practices in mind, we need to ensure that we split the data into train and test sets. This step will help prevent overfitting. Completing this step prior to training all of the models allows us to use the same train and test data across models. Note that we are using the shuffle feature to rearrange the recipes (in case the order was not originally random) and test_size=0.2 indicating that we want 80% of the data reserved for training and 20% for testing.\n",
        "\n",
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train, test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, shuffle=True, random_state=42)\n",
        "#DECISION TREE\n",
        "#The first model that we fit is a basic, unpruned decision tree. We use this model as a baseline for performance in the classification task.\n",
        "\n",
        " # Import decision tree from sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        " # Set up the decision tree\n",
        " clf = DecisionTreeClassifier(max_features=5000)\n",
        "\n",
        " # Fit the decision tree to the training data\n",
        " clf.fit(X_train, y_train)\n",
        "\n",
        "# Use the decision tree to predict values for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and print the results\n",
        "a = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score in % : \")\n",
        "print(a * 100)\n",
        "#For this first decision tree, the potentially unbiased test error is estimated to be 60.68%. For context, a human typically can classify recipes into the correct cuisine in 45-50% of attempts. The max-depth of this decision tree was 403 splits which could indicate overfitting. Ideally, we would tune the max-depth hyperparameter but since we only need a baseline, this number will suffice.\n",
        "\n",
        "#RANDOM FOREST\n",
        "#The second model chosen is an ensemble method known as ‘random forest’. You can read more about it on Wikipedia. While the decision tree serves only as a baseline classifier, with the Random Forest we want to tune the model’s hyper-parameters. For example, we tuned each of the following independently and then also used them as a basis for tuning a combination: maximum tree depth, number of trees in the forest, maximum number of features considered at each split, and minimum number of samples per split. The hyperparameter tuning code is not shown below but can be provided on request.\n",
        "\n",
        " # Import random forest classifier from sklearn\n",
        " from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        " # Set up random forest classifier\n",
        " clf = RandomForestClassifier()\n",
        "\n",
        " # Train the random forest (use ravel to coerce to 1d array)\n",
        " clf.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get test predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Get accuracy for the random forest classifier\n",
        "a = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score in % : \")\n",
        "print(a * 100)\n",
        "#By tuning the random forest, we were able to increase test accuracy from 67.11% to 71.64% by setting max_depth=200, n_estimators=250, max_features=‘sqrt’, and min_samples_split=7.\n",
        "\n",
        "# Setting up the tuned random forest\n",
        "clf = RandomForestClassifier(max_depth=200, n_estimators=250, max_features='sqrt', min_samples_split=7)\n",
        "#After training the random forest model, we can extract information about the relative importance of each feature (ingredient) in determining the class (cuisine) of a given recipe. The variable importance plot below shows how acai juice and nori furikake are considered distinguishing ingredients.\n",
        "\n",
        "\n",
        "\n",
        "#MULTINOMIAL LOGISTIC REGRESSION\n",
        "#To compete with the random forest, we trained a multinomial logistic regression.\n",
        "\n",
        " # import logistic regresion\n",
        " from sklearn.linear_model import LogisticRegression\n",
        " from sklearn.metrics import accuracy_score\n",
        "\n",
        " # Set up and fitlogistic regression\n",
        " clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train.ravel())\n",
        "\n",
        " # Get predictions on test data\n",
        " y_pred = clf.predict(X_test)\n",
        "\n",
        "# Get accuracy\n",
        "a = accuracy_score(y_test, y_pred)\n",
        "print(\"Appropriate tests in % : \")\n",
        "print(a * 100)\n",
        "#We were surprised by the performance of the logistic regression because it scored 78.14% test accuracy (6.5% better than the random forest). We believe that the number of features and sparseness of data is problematic for the random forest algorithm.\n",
        "\n",
        "\n",
        "#CUISINE CLUSTERING\n",
        "#In order to perform clustering at the cuisine level, we must aggregate the recipes to cuisine levels.\n",
        "\n",
        " # Group by cuisine and aggregate the data\n",
        " data_agg = df.groupby('cuisine').apply(lambda x: x.sum())\n",
        " data_agg = data_agg.drop(columns=['cuisine','id'])\n",
        " data_agg = data_agg.reset_index()\n",
        "\n",
        " ## Get all of the unique ingredients as features\n",
        " features_all_list = []\n",
        "\n",
        " for i in df.ingredients:\n",
        "    features_all_list += i\n",
        "\n",
        "    features = list(set(features_all_list))\n",
        "    len(features)\n",
        "\n",
        " onehot_ingredients = np.zeros((data_agg.shape[0], len(features)))\n",
        "  feature_lookup = sorted(features)\n",
        "#After applying tf-idf vectorization to standardize the data and principle components analysis (PCA) to reduce the dimensionality of the data (neither shown),we can apply a clustering algorithm. For simplicity and since we have labeled data, we chose K-Means. Other options such as Gaussian mixture models and hierarchical clustering could improve the clusters but were determined not to be necessary for this clustering task.\n",
        "\n",
        " # Import Kmeans clustering\n",
        " from sklearn.cluster import KMeans\n",
        "\n",
        " # Set # of clusters\n",
        " ## We tried 3, 4, 5, 6, 7, 8, and 10 with 5 being the best\n",
        " numOfClusters = 5\n",
        "\n",
        " # Set up KMeans\n",
        "kmeans = KMeans(init='k-means++', n_clusters=numOfClusters, n_init=10)\n",
        "# Fit kmeans\n",
        "kmeans.fit(reduced_data)\n",
        "\n",
        "# Predict kmeans\n",
        "kmeans_pred = kmeans.predict(reduced_data)\n",
        "kmeans_pred = kmeans_pred + 1\n",
        "kmeans = kmeans.fit(reduced_data)\n",
        "\n",
        "kmeans.predict.aggregate(columns_stack)\n",
        "kmeans.predict.feature_lookup()\n",
        "\n",
        "#Generate plot\n",
        "\n",
        "17# Generate plot of the resultant clusters\n",
        "x = reduced_data[:, 0]\n",
        "y = reduced_data[:, 1]\n",
        "\n",
        "# Set font size\n",
        "plt.rcParams.update({'font.size':15\n",
        "                    })\n",
        "\n",
        "# Get fig, ax, and set figure size\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "\n",
        "# Scatter the cuisines\n",
        "ax.scatter(x, y, s=5000, c=kmeans_pred, cmap='Set3')\n",
        "# Add labels to each cuisine\n",
        "for i, txt in enumerate(data_agg.cuisine):\n",
        "    ax.annotate(txt, (x[i], y[i]))"
      ]
    }
  ]
}